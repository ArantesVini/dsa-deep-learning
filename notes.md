# Chapter 2

## Short History of Neural Networks

- The history of the neural netowrks can be traced back to the 1940s with the first ideas, and, in 1958 with the perceptron.

- But in 1969, two researchers, Minsky and Papert, published a book called "Perceptrons" that showed the limitations of the perceptron, which led to a decline in interest in neural networks, the so-called "AI winter".

- In 1986, a new algorithm called backpropagation was introduced, which allowed for training of multi-layer neural networks, and this led to a resurgence of interest in neural networks. We still use backpropagation today.

- In 1989, Elman introduced the Recurrent Neural Network (RNN), which is a type of neural network that can process sequences of inputs. This was a major breakthrough for Natural Language Processing.

- In 1997, Hochreiter and Schmidhuber introduced the Long Short-Term Memory (LSTM) cell, which is a type of RNN that can learn long-term dependencies. This was another major breakthrough for NLP.

- In 2012, AlexNet, a deep convolutional neural network, won the ImageNet competition by a large margin, which was a major breakthrough for computer vision.

- In 2017 is published the papper "Attention is All You Need" that introduced the Transformer architecture, which is a type of neural network that can process sequences of inputs in parallel, which is much faster than RNNs.

- In 2020, the first Large Language Model, GPT-3, was released, which is a type of Transformer that can generate human-like text, with billions of parameters, resulting in impressive results on language generation.

- In 2023 a web interface was created to interact with GPT-3, called "Talk to Transformer", which allows users to generate text by typing a prompt.

## Common Deep Learning Architectures

- Dense Neural Networks (DNN)

- Convolutional Neural Networks (CNN)

- Recurrent Neural Networks (RNN)

- Autoenconders

- Generative Adversarial Networks (GANs)

- Architectures that are still on the academic side: Capsule Networks, Transformers, Graph Neural Networks, siamese networks, etc.

- Attention models and Transformers

## Frameworks and tools to build deep learning models

- PyTorch
- TensorFlow and Keras

Both based on Python and C++, the course focus.

Alternatives are: MxNet, JAX, and Open Neural Network Exchange (ONNX).

Libraries to mathematic oprations in C++ to train Deep Learning models: Armadillo, and MLpack.
