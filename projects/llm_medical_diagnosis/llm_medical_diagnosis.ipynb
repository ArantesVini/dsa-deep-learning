{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-i9OPHau2j8-"
      },
      "outputs": [],
      "source": [
        "!pip install -q accelerate peft bitsandbytes transformers trl datasets torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hsRrYrWO7UAj"
      },
      "outputs": [],
      "source": [
        "# Imports\n",
        "import accelerate\n",
        "import peft\n",
        "import bitsandbytes\n",
        "import transformers\n",
        "import trl\n",
        "import datasets\n",
        "\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
        "from transformers import TrainingArguments\n",
        "from peft import AutoPeftModelForCausalLM, LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
        "from trl import SFTTrainer\n",
        "import warnings\n",
        "\n",
        "\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KkLh0rmMi5g4"
      },
      "outputs": [],
      "source": [
        "if torch.cuda.is_available():\n",
        "    print(\"GPUs number:\", torch.cuda.device_count())\n",
        "    print(\"GPU Model:\", torch.cuda.get_device_name(0))\n",
        "    print(\"GPU Total Memory [GB]:\",torch.cuda.get_device_properties(0).total_memory / 1e9)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wo2FsF4Zi5g4"
      },
      "outputs": [],
      "source": [
        "# GPU Memory reset (If needed)\n",
        "from numba import cuda\n",
        "device = cuda.get_current_device()\n",
        "device.reset()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XnC-vkUb2j8_"
      },
      "outputs": [],
      "source": [
        "ft_dataset = load_dataset(\"nlpie/Llama2-MedTuned-Instructions\")\n",
        "\n",
        "# Using only a small part of the total dataset\n",
        "# In a professional enviroment, we could use this to test the viability of the fine-tunning\n",
        "# So we could first train with an small sample\n",
        "# And then, proceed to train with all data, which will take way more time\n",
        "ft_dataset[\"train\"] = ft_dataset[\"train\"].select(range(3500))\n",
        "ft_dataset[\"test\"] = ft_dataset[\"train\"].select(range(300))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iatbXjLY2j8_"
      },
      "outputs": [],
      "source": [
        "# Prompt is simply the input text, just a fancy name for it\n",
        "def create_prompt(sample):\n",
        "    prompt = sample[\"instruction\"]\n",
        "    prompt += sample[\"input\"]\n",
        "    single_turn_prompt = f\"Instruction: {prompt}<|end_of_turn|>AI Assistant: {sample['output']}\"\n",
        "\n",
        "    return single_turn_prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rsmIQBXj2j8_"
      },
      "outputs": [],
      "source": [
        "# Quantization configuration for the LM\n",
        "# Will use 4-bit quantization, to minimize memory usage\n",
        "# While maintaing reasonable performance\n",
        "bnb_config = BitsAndBytesConfig(load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=\"float16\",\n",
        "    bnb_4bit_use_double_quant=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# LLM Based on Mistral-7b\n",
        "# Trained only in english\n",
        "hf_repo = \"berkeley-nest/Starling-LM-7B-alpha\"\n",
        "\n",
        "llm_model = AutoModelForCausalLM.from_pretrained(hf_repo,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\",\n",
        "    use_cache=False)"
      ],
      "metadata": {
        "id": "SpVRmSa_1Tus"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(hf_repo)\n",
        "\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"right\""
      ],
      "metadata": {
        "id": "D0Gt4Jiu2f6o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hkyX9j1D2j9A"
      },
      "outputs": [],
      "source": [
        "def generate_answer_before_fine_tunning(prompt, model):\n",
        "    encoded_input = tokenizer(prompt,\n",
        "        return_tensors=\"pt\",\n",
        "        add_special_tokens=True,\n",
        "    )\n",
        "\n",
        "    model_inputs = encoded_input.to(\"cuda\")\n",
        "    generated_ids = model.generate(**model_inputs,\n",
        "        max_new_tokens=1024,\n",
        "        do_sample=True,\n",
        "        pad_token_id=tokenizer.eos_token_id,\n",
        "    )\n",
        "\n",
        "    decoded_output = tokenizer.batch_decode(generated_ids)\n",
        "\n",
        "    return decoded_output[0].replace(prompt, \"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X2KbrU4M2j9A"
      },
      "outputs": [],
      "source": [
        "prompt = \"\"\"Instruction: Your goal is to determine the relationship between the two provided clinical sentences and classify them into one of the following categories:\n",
        "Contradiction: If the two sentences contradict each other. Neutral: If the two sentences are unrelated to each other. Entailment: If one of the sentences logically entails the other. \"\"\"\n",
        "prompt += '''Sentence 1: For his hypotension, autonomic testing confirmed orthostatic hypotension. Sentence 2: the patient has orthostatic hypotension <|end_of_turn|>'''\n",
        "prompt += \"AI Assistant:\"\n",
        "\n",
        "generate_answer_before_fine_tunning(prompt, llm_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HqOlyUpQ2j9A"
      },
      "outputs": [],
      "source": [
        "# LoRA (Low-Rank Adaptation):\n",
        "## A technique for adapting pre-trained models by adding low-rank weight updates,\n",
        "## reducing memory and compute costs.\n",
        "\n",
        "# PEFT (Parameter-Efficient Fine-Tuning):\n",
        "## A method to fine-tune large models efficiently by modifying only a small subset of parameters,\n",
        "## improving adaptability with minimal resource usage.\n",
        "peft_config = LoraConfig(r=8,\n",
        "    lora_alpha=16,\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        ")\n",
        "\n",
        "llm_model = prepare_model_for_kbit_training(llm_model)\n",
        "\n",
        "llm_model = get_peft_model(llm_model, peft_config)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from trl import SFTConfig"
      ],
      "metadata": {
        "id": "BvoO1HDk8tXT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_arguments = SFTConfig(output_dir=\"adjusted_model\",\n",
        "    per_device_train_batch_size=1,\n",
        "    gradient_accumulation_steps=4,\n",
        "    optim=\"paged_adamw_32bit\",\n",
        "    learning_rate=2e-4,\n",
        "    lr_scheduler_type=\"cosine\",\n",
        "    save_strategy=\"epoch\",\n",
        "    logging_steps=10,\n",
        "    num_train_epochs=1,\n",
        "    max_steps=250,\n",
        "    fp16=True,\n",
        "    max_seq_length=512,\n",
        "    packing=True\n",
        ")\n",
        "\n",
        "trainer = SFTTrainer(model=llm_model,\n",
        "    peft_config=peft_config,\n",
        "    tokenizer=tokenizer,\n",
        "    formatting_func=create_prompt,\n",
        "    args=training_arguments,\n",
        "    train_dataset=ft_dataset[\"train\"],\n",
        "    eval_dataset=ft_dataset[\"test\"],\n",
        ")"
      ],
      "metadata": {
        "id": "2KjzPpDl6Ki2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CdQ8yQcH2j9A"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DRkrNXsW2j9A"
      },
      "outputs": [],
      "source": [
        "trainer.save_model(\"adjusted_model\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jewVrAtA2j9B"
      },
      "outputs": [],
      "source": [
        "final_model = llm_model.merge_and_unload()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jbavws8b2j9B"
      },
      "outputs": [],
      "source": [
        "def generate_answer_after_fine_tunning(prompt, model):\n",
        "    encoded_input = tokenizer(prompt,\n",
        "        return_tensors=\"pt\",\n",
        "        add_special_tokens=True,\n",
        "    )\n",
        "\n",
        "    model_inputs = encoded_input.to(\"cuda\")\n",
        "    generated_ids = model.generate(**model_inputs,\n",
        "        max_new_tokens=512,\n",
        "        do_sample=True,\n",
        "        use_cache=False,\n",
        "        pad_token_id=tokenizer.eos_token_id,\n",
        "    )\n",
        "\n",
        "    decoded_output = tokenizer.batch_decode(generated_ids)\n",
        "\n",
        "    return decoded_output[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ieOBWhHH2j9B"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "prompt = \"Instruction: In your role as a medical professional, address the user's medical questions and concerns. \"\n",
        "prompt += \"I have a white tab under my tounge that is not only painful when i touch it but bleeds as well. not sure what it is, or why I got it. Can you give me any advise? <|end_of_turn|> \"\n",
        "prompt += \"\\nAI Assistant:\"\n",
        "response = generate_answer_after_fine_tunning(prompt, final_model)\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "auKNgQEO2j9F"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "prompt = \"Instruction: In your capacity as a healthcare expert, offer insights and recommendations in response to users' medical inquiries. \"\n",
        "prompt += \"I have terrible anxiety and depression. I've tried various therapists and pills, but nothing's helped. <|end_of_turn|> \"\n",
        "prompt += \"\\nAI Assistant:\"\n",
        "response = generate_answer_after_fine_tunning(prompt, final_model)\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "osJTWGpw2j9F"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "prompt = \"Instruction: As a medical chatbot, your responsibility is to provide information and guidance on medical matters to users. \"\n",
        "prompt += \"Hi sir, I am so happy with this website. First of all thanks for giving this opportunity. I am the  Software employee.My age is 24. My height is 169cm .Recently I got back pain and some pain in chest. How can i get relief from those pains.How i improve my health and which type of diseases will attack to my life in future. Please give Some health tips for heart and kidneys protection. <|end_of_turn|> \"\n",
        "prompt += \"\\nAI Assistant:\"\n",
        "response = generate_answer_after_fine_tunning(prompt, final_model)\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zpbp_hoVJr-6"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}